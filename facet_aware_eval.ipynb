{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence indices mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:11:57.476012Z",
     "start_time": "2020-04-28T21:11:57.467837Z"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "\n",
    "def hashhex(s):\n",
    "    \"\"\"Returns a heximal formated SHA1 hash of the input string.\"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "    return [hashhex(url) for url in url_list]\n",
    "\n",
    "\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file) as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip().encode('utf8'))\n",
    "    return lines\n",
    "\n",
    "\n",
    "def map_cnndailymail(url_file):\n",
    "    url_list = read_text_file(url_file)\n",
    "    url_hashes = get_url_hashes(url_list)\n",
    "    story_fnames = [s + \".story\" for s in url_hashes]\n",
    "    num_stories = len(story_fnames)\n",
    "    print(f'num_stories={num_stories}')\n",
    "    return story_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:11:58.392945Z",
     "start_time": "2020-04-28T21:11:58.361169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_stories=11490\n"
     ]
    }
   ],
   "source": [
    "story_fnames = map_cnndailymail('data/all_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:12:00.209560Z",
     "start_time": "2020-04-28T21:12:00.204894Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2fname = {ct: fname for ct, fname in enumerate(story_fnames)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:12:05.668856Z",
     "start_time": "2020-04-28T21:12:05.659211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'469c6ac05092ca5997728c9dfc19f9ab6b936e40.story'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the original id of CNN/DM\n",
    "idx2fname[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:14:09.922007Z",
     "start_time": "2020-04-28T21:14:09.914610Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_one_cnndm_new(idx):\n",
    "    # please refer to https://github.com/ChenRocks/fast_abs_rl to generate json files for each sample\n",
    "    # we use their sentence tokenization results in the annotation\n",
    "    with open(f'./cnn-dailymail/finished_files/test/{idx}.json') as f:\n",
    "        data = json.loads(f.read())\n",
    "    return data['article'], data['abstract']\n",
    "\n",
    "\n",
    "def match_extract_idx_all(FAMs):\n",
    "    '''\n",
    "    given raw output, find the indices of extracted sentences\n",
    "    '''\n",
    "    # sample idx to indices of extracted sentences\n",
    "    idx2labels = defaultdict(list)\n",
    "    for idx in FAMs['low_abs']:\n",
    "        fname = idx2fname[idx][:-6]\n",
    "        # feed your model output here\n",
    "        with open(f\"../your_model_output/{fname}.model\") as f:\n",
    "            ext_l = [line.strip().lower() for line in f if len(line) > 1]\n",
    "        match_extract_idx(idx, ext_l, idx2labels)\n",
    "    return idx2labels\n",
    "\n",
    "\n",
    "def match_extract_idx(idx, ext_l, idx2labels):\n",
    "    '''\n",
    "    since the results of sentence tokenization might vary by models, we find the closest matched sentence by TF-IDF\n",
    "    to match the sentence idx used in our annotation.\n",
    "    idx: the idx of sample in the test set\n",
    "    ext_l: a list of sentence str representing the extracted summary\n",
    "    '''\n",
    "    tfidf = TfidfVectorizer()\n",
    "    sent_l, _ = read_one_cnndm_new(idx)\n",
    "    doc_vec_all = tfidf.fit_transform(sent_l)\n",
    "    for sent in ext_l:\n",
    "        sent_vec = tfidf.transform([sent])\n",
    "        scores = cosine_similarity(sent_vec, doc_vec_all).squeeze()\n",
    "        rank = np.argsort(-scores)\n",
    "        idx2labels[idx].append(rank[0])\n",
    "        # make sure they are (almost) identical\n",
    "        print('Original sentence', sent)\n",
    "        print('Found sentence', sent_l[rank[0]])\n",
    "        print()\n",
    "    print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facet-Aware Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:14:10.881993Z",
     "start_time": "2020-04-28T21:14:10.698978Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:18:03.100560Z",
     "start_time": "2020-04-28T21:18:03.086712Z"
    }
   },
   "outputs": [],
   "source": [
    "def facet_aware_eval(FAMs, method='lead3', num_sent_max=100, num_sup_group_max=100):\n",
    "    def calc_prec(p, g):\n",
    "        return len(p & g) / len(p)\n",
    "\n",
    "    def calc_recall(p, g):\n",
    "        return len(p & g) / len(g)\n",
    "\n",
    "    def calc_f1(p, g):\n",
    "        corr = len(p & g)\n",
    "        P = corr / len(p)\n",
    "        R = corr / len(g)\n",
    "        return 2 * P * R / (P + R + 1e-6)\n",
    "\n",
    "    def calc_facet_recall(p, sup_dict, n_sup=100):\n",
    "        cover = 0\n",
    "        for ref_i in sup_dict:\n",
    "            for sup_group in sup_dict[ref_i][:n_sup]:\n",
    "                if len(set(sup_group) - p) == 0:\n",
    "                    cover += 1\n",
    "                    break\n",
    "        return cover / len(sup_dict)\n",
    "\n",
    "    f1_l = []\n",
    "    prec_l = []\n",
    "    SAR_l = []\n",
    "    FAR_l = []\n",
    "    for idx in FAMs['low_abs']:\n",
    "        # a set of salient (support) sentences\n",
    "        all_sup_sents = set([i for ref in FAMs['low_abs'][idx]\n",
    "                             for sup_group in FAMs['low_abs'][idx][ref][:num_sup_group_max] for i in sup_group])\n",
    "        if method == 'lead3':\n",
    "            ext_sents = set(range(3))\n",
    "        elif method == 'fast_ext_rl':\n",
    "            ext_sents = set(idx2labels_fastrl[idx][:num_sent_max])\n",
    "        elif method == 'refresh':\n",
    "            ext_sents = set(idx2labels_refresh[idx][:num_sent_max])\n",
    "        elif method == 'neusum':\n",
    "            ext_sents = set(idx2labels_neusum[idx][:num_sent_max])\n",
    "        elif method == 'banditsum':\n",
    "            ext_sents = set(idx2labels_bs[idx][:num_sent_max])\n",
    "        elif method == 'unified_extract':\n",
    "            ext_sents = set(idx2labels_unified[idx][:num_sent_max])\n",
    "        # TODO add your method here\n",
    "        else:\n",
    "            raise\n",
    "        assert len(ext_sents) != 0\n",
    "\n",
    "        prec_l.append(calc_prec(ext_sents, all_sup_sents))\n",
    "        SAR_l.append(calc_recall(ext_sents, all_sup_sents))\n",
    "        f1_l.append(calc_f1(ext_sents, all_sup_sents))\n",
    "        FAR_l.append(calc_facet_recall(\n",
    "            ext_sents, FAMs['low_abs'][idx], num_sup_group_max))\n",
    "\n",
    "    P, SAR, F1, FAR = np.mean(prec_l), np.mean(\n",
    "        SAR_l), np.mean(f1_l), np.mean(FAR_l)\n",
    "    print(f'{method:15} #samples: {len(f1_l)} SAP: {P:.3f} SAR: {SAR:.3f} SAF1: {F1:.3f} FAR: {FAR:.3f}')\n",
    "    return P, SAR, F1, FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load FAMs and system outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:18:03.883257Z",
     "start_time": "2020-04-28T21:18:03.869082Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2labels_unified = pickle.load(open('data/idx2labels_unified.pkl', 'rb'))\n",
    "idx2labels_neusum = pickle.load(open('data/idx2labels_neusum.pkl', 'rb'))\n",
    "idx2labels_bs = pickle.load(open('data/idx2labels_bs.pkl', 'rb'))\n",
    "idx2labels_refresh = pickle.load(open('data/idx2labels_refresh.pkl', 'rb'))\n",
    "idx2labels_fastrl = pickle.load(open('data/idx2labels_fastrl.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T22:32:47.824152Z",
     "start_time": "2020-04-28T22:32:47.819643Z"
    }
   },
   "outputs": [],
   "source": [
    "FAMs = pickle.load(open('data/FAMs.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T22:32:52.170713Z",
     "start_time": "2020-04-28T22:32:52.167462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_abs 89\n",
      "noise 41\n",
      "high_abs 20\n",
      "all_idx 150\n"
     ]
    }
   ],
   "source": [
    "for k in FAMs:\n",
    "    print(k, len(FAMs[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T22:32:54.781767Z",
     "start_time": "2020-04-28T22:32:54.778058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [{0}, {2}], 1: [{3, 5}]})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Facet 0 has two support groups {0}, {2}\n",
    "# Facet 1 has one support group {3, 5}\n",
    "FAMs['low_abs'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T22:36:27.351119Z",
     "start_time": "2020-04-28T22:36:27.346911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_idx: 0 facet-0 sup_group: {1}\n",
      "sample_idx: 0 facet-1 sup_group: {19}\n",
      "sample_idx: 0 facet-2 sup_group: {25}\n",
      "\n",
      "sample_idx: 1 facet-0 sup_group: {0}\n",
      "sample_idx: 1 facet-0 sup_group: {2}\n",
      "sample_idx: 1 facet-1 sup_group: {3, 5}\n",
      "\n",
      "sample_idx: 2 facet-0 sup_group: {13}\n",
      "sample_idx: 2 facet-1 sup_group: {0}\n",
      "sample_idx: 2 facet-1 sup_group: {1}\n",
      "sample_idx: 2 facet-2 sup_group: {6, 14}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in FAMs['low_abs']:\n",
    "    for facet, sup_groups in FAMs['low_abs'][idx].items():\n",
    "        for sup_group in sup_groups:\n",
    "            print(f'sample_idx: {idx} facet-{facet} sup_group: {sup_group}')\n",
    "    print()\n",
    "    if idx > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:14:36.203329Z",
     "start_time": "2020-04-28T21:14:36.198172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 105,\n",
       " 379,\n",
       " 420,\n",
       " 438,\n",
       " 472,\n",
       " 524,\n",
       " 678,\n",
       " 791,\n",
       " 835,\n",
       " 852,\n",
       " 977,\n",
       " 1249,\n",
       " 1560,\n",
       " 1683,\n",
       " 2023,\n",
       " 2043,\n",
       " 2085,\n",
       " 2371,\n",
       " 2521,\n",
       " 2542,\n",
       " 2644,\n",
       " 2725,\n",
       " 2807,\n",
       " 2843,\n",
       " 3143,\n",
       " 3183,\n",
       " 3214,\n",
       " 3479,\n",
       " 3545,\n",
       " 3594,\n",
       " 3643,\n",
       " 3704,\n",
       " 4148,\n",
       " 4177,\n",
       " 4189,\n",
       " 4223,\n",
       " 4249,\n",
       " 4742,\n",
       " 5005,\n",
       " 5368,\n",
       " 5386,\n",
       " 5388,\n",
       " 5771,\n",
       " 5814,\n",
       " 5941,\n",
       " 5978,\n",
       " 6018,\n",
       " 6056,\n",
       " 6079,\n",
       " 6525,\n",
       " 6571,\n",
       " 6582,\n",
       " 6586,\n",
       " 6626,\n",
       " 6729,\n",
       " 6839,\n",
       " 6842,\n",
       " 6846,\n",
       " 6852,\n",
       " 6886,\n",
       " 7164,\n",
       " 7168,\n",
       " 7331,\n",
       " 7360,\n",
       " 7428,\n",
       " 7435,\n",
       " 7494,\n",
       " 7570,\n",
       " 7579,\n",
       " 7675,\n",
       " 7709,\n",
       " 7784,\n",
       " 8043,\n",
       " 8061,\n",
       " 8066,\n",
       " 8093,\n",
       " 8157,\n",
       " 8219,\n",
       " 8226,\n",
       " 8249,\n",
       " 8299,\n",
       " 8513,\n",
       " 8799,\n",
       " 8925,\n",
       " 9166,\n",
       " 9711,\n",
       " 9715,\n",
       " 9980,\n",
       " 10128,\n",
       " 10290,\n",
       " 10325,\n",
       " 10395,\n",
       " 10636,\n",
       " 10681,\n",
       " 10686,\n",
       " 10739,\n",
       " 11269,\n",
       " 11364,\n",
       " 11388,\n",
       " 11395]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample id list\n",
    "FAMs['all_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T21:18:09.304080Z",
     "start_time": "2020-04-28T21:18:09.294377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead3           #samples: 89 SAP: 0.610 SAR: 0.373 SAF1: 0.445 FAR: 0.506\n",
      "fast_ext_rl     #samples: 89 SAP: 0.648 SAR: 0.406 SAF1: 0.479 FAR: 0.508\n",
      "banditsum       #samples: 89 SAP: 0.586 SAR: 0.343 SAF1: 0.417 FAR: 0.447\n",
      "neusum          #samples: 89 SAP: 0.639 SAR: 0.395 SAF1: 0.468 FAR: 0.512\n",
      "refresh         #samples: 89 SAP: 0.610 SAR: 0.375 SAF1: 0.447 FAR: 0.513\n",
      "unified_extract #samples: 89 SAP: 0.669 SAR: 0.413 SAF1: 0.488 FAR: 0.548\n"
     ]
    }
   ],
   "source": [
    "# results in the paper (note that only low_abs samples can be evaluated)\n",
    "for method in ['lead3', 'fast_ext_rl', 'banditsum', 'neusum', 'refresh', 'unified_extract']:\n",
    "    res = facet_aware_eval(FAMs=FAMs, method=method, num_sent_max=3)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
